{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c876022",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import imutils\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image, ImageEnhance\n",
    "import seaborn as sns\n",
    "\n",
    "MODEL_NAME = \"tumor\"\n",
    "EPOCHS = 100\n",
    "BATCH_SIZE = 16\n",
    "IMG_WIDTH = 100\n",
    "IMG_HEIGHT = 100\n",
    "TUMOR_CLASSES = [\"category1_tumor\", \"category2_tumor\", \"category3_tumor\", \"no_tumor\"]\n",
    "TEST_SIZE = 0.4\n",
    "DIRECTORY = \"../Datathon-Dataset\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "734863a1",
   "metadata": {},
   "source": [
    "### Enhance Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "479e7542",
   "metadata": {},
   "outputs": [],
   "source": [
    "def enhance_image(img):\n",
    "\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "    gray = cv2.GaussianBlur(gray, (3, 3), 0)\n",
    "\n",
    "    # threshold the image, then perform a series of erosions +\n",
    "    # dilations to remove any small regions of noise\n",
    "    thresh = cv2.threshold(gray, 45, 255, cv2.THRESH_BINARY)[1]\n",
    "    thresh = cv2.erode(thresh, None, iterations=2)\n",
    "    thresh = cv2.dilate(thresh, None, iterations=2)\n",
    "\n",
    "    # find contours in thresholded image, then grab the largest one\n",
    "    cnts = cv2.findContours(thresh.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    cnts = imutils.grab_contours(cnts)\n",
    "    c = max(cnts, key=cv2.contourArea)\n",
    "\n",
    "    # find the extreme points\n",
    "    extLeft = tuple(c[c[:, :, 0].argmin()][0])\n",
    "    extRight = tuple(c[c[:, :, 0].argmax()][0])\n",
    "    extTop = tuple(c[c[:, :, 1].argmin()][0])\n",
    "    extBot = tuple(c[c[:, :, 1].argmax()][0])\n",
    "    ADD_PIXELS = 0\n",
    "    new_img = img[extTop[1]-ADD_PIXELS:extBot[1]+ADD_PIXELS, extLeft[0]-ADD_PIXELS:extRight[0]+ADD_PIXELS].copy()\n",
    "    \n",
    "    return new_img\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5227fbb2",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108f6814",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(base_path):\n",
    "\n",
    "    images = []\n",
    "    labels = []\n",
    "\n",
    "    for i, tumor_class in enumerate(TUMOR_CLASSES):\n",
    "        class_path = os.path.join(base_path, tumor_class)\n",
    "\n",
    "        for filename in os.listdir(class_path):\n",
    "            if filename.endswith(\".jpg\"):\n",
    "                img = cv2.imread(os.path.join(class_path, filename))\n",
    "                img = enhance_image(img)\n",
    "                img = cv2.resize(img, (IMG_WIDTH, IMG_HEIGHT))\n",
    "                images.append(img)\n",
    "                labels.append(i)\n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef418817",
   "metadata": {},
   "source": [
    "### Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f8bbe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    layers = [\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Conv2D(\n",
    "            128, (3, 3), activation=\"relu\", input_shape=(IMG_WIDTH, IMG_HEIGHT, 3),padding='same'),\n",
    "        tf.keras.layers.MaxPooling2D(pool_size=(2, 2),padding='same'),\n",
    "\n",
    "        tf.keras.layers.Conv2D(256, (3, 3), activation=\"relu\",padding='same'),\n",
    "        tf.keras.layers.MaxPooling2D(pool_size=(2, 2),padding='same'),\n",
    "\n",
    "        tf.keras.layers.Conv2D(512, (3, 3), activation=\"relu\",padding='same'),\n",
    "        tf.keras.layers.MaxPooling2D(pool_size=(2, 2),padding='same'),\n",
    "\n",
    "        tf.keras.layers.Conv2D(512, (3, 3), activation=\"relu\",padding='same'),\n",
    "        tf.keras.layers.MaxPooling2D(pool_size=(2, 2),padding='same'),\n",
    "\n",
    "        tf.keras.layers.Conv2D(512, (3, 3), activation=\"relu\",padding='same'),\n",
    "        tf.keras.layers.MaxPooling2D(pool_size=(2, 2),padding='same'),\n",
    "\n",
    "        # Flatten units\n",
    "        tf.keras.layers.Flatten(),\n",
    "\n",
    "        # Add hidden layers with dropout\n",
    "        tf.keras.layers.Dense(512, activation=\"relu\"),\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "\n",
    "        # Output layer\n",
    "        tf.keras.layers.Dense(4, activation=\"softmax\"),\n",
    "\n",
    "    ]\n",
    "\n",
    "    model = tf.keras.models.Sequential(layers)\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
    "    model.compile(optimizer=optimizer,\n",
    "                  loss=\"categorical_crossentropy\",\n",
    "                  metrics=[\"accuracy\"])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "916141d5",
   "metadata": {},
   "source": [
    "### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea7d0148",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure GPU memory growth\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "\n",
    "# Seed for reproducibility\n",
    "tf.random.set_seed(123)\n",
    "\n",
    "# Get image arrays and labels for all image files\n",
    "images, labels = load_data(DIRECTORY)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "labels = tf.keras.utils.to_categorical(labels)\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    np.array(images), np.array(labels), test_size=TEST_SIZE\n",
    ")\n",
    "\n",
    "# Get a compiled neural network\n",
    "model = get_model()\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=15,restore_best_weights=True, verbose=2)\n",
    "reduce_learning = ReduceLROnPlateau(monitor = 'val_loss',factor = 0.3,patience = 5,verbose = 1,mode = 'min')\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=10,       # Rotate images by up to 10 degrees\n",
    "    width_shift_range=0.05,   # Shift images horizontally by up to 20% of the width\n",
    "    height_shift_range=0.05,  # Shift images vertically by up to 20% of the height\n",
    "    zoom_range=0.05,          # Zoom in by up to 20% on some images\n",
    "    shear_range=0.05,         # Shear by up to 20% on some images\n",
    "    brightness_range=[0.5,1.5], # Brightness range\n",
    "    horizontal_flip=True,    # Flip images horizontally\n",
    ")\n",
    "\n",
    "augmented_train_data = datagen.flow(x_train, y_train, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "260e488d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit model on training data\n",
    "start_time = time.time()\n",
    "history = model.fit(augmented_train_data, epochs=EPOCHS, callbacks=[early_stopping,reduce_learning], validation_split=0.2,\n",
    "                    validation_data=(x_test, y_test), verbose=2)\n",
    "end_time = time.time()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb1c14b",
   "metadata": {},
   "source": [
    "### Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0affe548",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, accuracy = model.evaluate(x_test, y_test, verbose=2)\n",
    "predictions = model.predict(x_test)\n",
    "y_pred = np.argmax(predictions, axis=1)\n",
    "y_test = np.argmax(y_test, axis=1)\n",
    "print(\"Time taken: \", end_time - start_time, \" seconds\")\n",
    "print(f\"\\nLoss: {loss}\")\n",
    "print(\"Accuracy: \", accuracy_score(y_test, y_pred))\n",
    "print(\"Recall: \", recall_score(y_test, y_pred, average='weighted'))\n",
    "print(\"Precision: \", precision_score(y_test, y_pred, average='weighted'))\n",
    "print(\"F1 Score: \", f1_score(y_test, y_pred, average='weighted'))\n",
    "\n",
    "confusion = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(confusion, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
    "            xticklabels=TUMOR_CLASSES, yticklabels=TUMOR_CLASSES)\n",
    "plt.xlabel(\"Predicted Labels\")\n",
    "plt.ylabel(\"True Labels\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb82ddd4",
   "metadata": {},
   "source": [
    "### Write to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d180ca",
   "metadata": {},
   "outputs": [],
   "source": [
    " with open('logs.txt', 'a') as f:\n",
    "        #write model summary and evaluation metrics to file\n",
    "        f.write(\"\\n\\n\\nModel Summary: \\n\")\n",
    "        model.summary(print_fn=lambda x: f.write(x + '\\n'))\n",
    "        f.write(\"\\nModel Name: \" + str(MODEL_NAME)\n",
    "        f.write(\"\\nImage Size: \" + str(IMG_WIDTH) + \"x\" + str(IMG_HEIGHT))\n",
    "        f.write(\"\\nBatch Size: \" + str(BATCH_SIZE))\n",
    "        f.write(\"\\nTime taken: \" + str(end_time - start_time) + \" seconds\" )\n",
    "        f.write(\"\\nLoss: \" + str(loss))\n",
    "        f.write(\"\\nAccuracy: \" + str(accuracy))\n",
    "        f.write(\"\\nRecall: \" + str(recall_score(y_test, y_pred, average='weighted')))\n",
    "        f.write(\"\\nPrecision: \" + str(precision_score(y_test, y_pred, average='weighted')))\n",
    "        f.write(\"\\nF1 Score: \" + str(f1_score(y_test, y_pred, average='weighted')))\n",
    "        f.write(\"_________________________________________________________________\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2471f5f6",
   "metadata": {},
   "source": [
    "### Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf79cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(MODEL_NAME)\n",
    "print(f\"Model saved to {MODEL_NAME}.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
